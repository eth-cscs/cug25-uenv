%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deployment}
\label{sec:deployment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{itemize}
\item the CI/CD tooling
\item the command line interface (CLI)
\item the slurm plugin
\end{itemize}

%------------------------------------------------------------------------------
\section{Stackinator: Building Software}
%------------------------------------------------------------------------------

%..............................................................................
\subsection{}
%..............................................................................

%..............................................................................
\subsection{OpenMPI}
%..............................................................................

\openmpi with support for libfabric is available through uenv on Alps.
The main motivation behind providing it is to have an alternative to \craympich, for when \craympich has application-specific performance problems or bugs, or users want to install a pre-compiled application that was linked against \openmpi.

In the paper we will describe how we configure \openmpi with libfabric, and show performance results in applications up to thousands of Grace-Hopper GPUs.
We will also show the recipes for building different versions of the open source distributions of libfabric and the CXI driver from source, and integrating them in a uenv.

%..............................................................................
\subsection{NVIDIA libraries}
%..............................................................................

NVIDIA provide a suite of mathematical and communication libraries for distributed computation.
Application developers are integrating these libraries, where they are required for the best performance

In particular, the \cufftmp and \cusolvermp libraries implement optimized distributed FFT and linear algebra solvers.
Examples of important applications that require these libraries for optimal performance are \cufftmp in GROMACS and \cusolvermp in VASP.
These two libraries use \nvshmem for distributed communication.
\nvshmem creates a global address space for data that spans the memory of multiple GPUs (both inter-node and intra-node) athat can be accessed via GPU-initiated operations.

Both \cufftmp and \cusolvermp are distrbuted as pre-compiled binary distributions.
\cufftmp in particular is challenging, because it is only distributed inside the NVIDIA HPC SDK, where it is linked against communication libraries that target UCX/Infiniband, not libfabric/Slingshot.

The paper will describe in detail how CSCS provides these packages:
\begin{itemize}
    \item create a custom spack package to the latest version \nvshmem with support for libfabric/Slingshot 11.
    \item extract \cufftmp from the NVIDIA HPC SDK.
    \item write Spack packages that install \cufftmp and \cusolvermp and patch them to use \nvshmem built for libfabric.
    \item customise the Spack packages for GROMACS.
\end{itemize}

%------------------------------------------------------------------------------
\section{Multi cluster/multi architecture}
%------------------------------------------------------------------------------

Alps is a HPE Cray EX system, with the five node node types described in \tbl{tab:alps-nodes}.
On top of the Alps infrastructure, CSCS deploys vClusters (versatile software defined clusters), where each vCluster is a separate SLURM vCluster that is customised for specific platforms.

Each vCluster is configured for different target communities, use cases or customers.
As such, the version of SLURM, mounted file systems and installed software can vary between vClusters.
Of particular interest to uenv, the version of \xpmemm and \libfabric) installed on the system.

The following characteristics define a uenv
\begin{itemize}
    \item name: the name of the uenv
    \item version: the version of the uenv
    \item tag: used to indicate multiple releases of the same uenv
    \item uarch: the node type for which the image is built
    \item system: the vCluster for which the image is built
\end{itemize}

For example: \lst{prgenv-gnu/24.7:v1\%gh200@daint}

Is the gnu programming environment, version 24.7, tag v1, built for the Grace-Hopper nodes on Daint.

\begin{table*}[h!]
\centering
\begin{tabular}{llrrrr}
\toprule
uarch   & type         & blades & nodes & CPU sockets & GPU devices \\
\midrule
gh200   & NVIDIA GH200 & 1,344   & 2,688  & 10,752      & 10,752      \\
zen2    & AMD Rome     & 256     & 1,024  & 2,048       & --          \\
a100    & NVIDIA A100  & 72      & 144    & 144         & 576         \\
mi300   & AMD MI300A   & 64      & 128    & 512         & 512         \\
mi200   & AMD MI250x   & 12      & 24     & 24          & 96          \\
\midrule
\multicolumn{2}{c}{\textsc{Total}}      & 1,748   & 3,880  & 13,480  & 11,936 \\
\bottomrule
\end{tabular}
\caption{Alps node types and their specifications.}
\label{tab:alps-nodes}
\end{table*}

%------------------------------------------------------------------------------
\section{The uenv CLI}
%------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Providing CPE}
\label{sec:cpe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Though CPE is not used as the foundation of software environments, the CPE itself is an environment that we want to provide to users of Alps.

HPE has been making good progress improving the modularity of the CPE and providing more flexible methods of installing the CPE.

This section will describe how CSCS takes advantage of these efforts to provide ``CPE in a container", that delivers a familiar CPE user experience, while addressing the key challenges:

\begin{itemize}
\item CPE containers can be built, tested and deployed by the support team without root access;
\item New releases, custom configurations and patched releases of CPE can be deployed without any changes to the underlying system or interruption to service;
\item Using a container engine with full integration with SLURM simplifies the user experience
\end{itemize}

With the integrated container engine on Alps, starting a job with a container mounted is straightforward.
For example, to start an interactive shell on a compute node with a specific release of CPE loaded:
\begin{lstlisting}
srun --environment=cpe/24.11 --pty bash
\end{lstlisting}

\section{Other Applications}

In this section we explore some special applications of uenv, that we use to enable specific workflows.

%------------------------------------------------------------------------------
\subsection{Standard Environment}
%------------------------------------------------------------------------------

There are a wealth of high-quality command line tools (e.g. fd, ripgrep and neovim), that are either not installed on HPC systems, or the version provided on the system is out of date or not configured properly (e.g. the version of vim provided by SUSE does not use treesitter).

We use uenv to provide a set of up to date and modern command line tools, that is permanantly mounted on Alps vClusters.

The package includes:
\begin{itemize}
    \item \emph{neovim} and \emph{emacs}, built with \emph{treesitter};
    \item \emph{fd} and \emph{ripgrep} (modern versions of the \lst{find} and \lst{grep} tools);
    \item \emph{oh-my-posh} (a custom command line prompt);
    \item programming languages: \emph{lua}, \emph{go}, \emph{rust} and \emph{python}.
\end{itemize}

The uenv is created using the uenv pipeline, however it is mounted ``permanantly'' via a vService:
\lstinputlisting[language=bash]{./code/manual-uenv-mount.sh}

Updated sets of tools can deployed without rebooting the system, and rolled back by mounting the old squashfs image if there is a problem.
It is also simpler to test out a new stack, by switching to the new image on a handful of reserved nodes, before rolling out to the whole system.

%------------------------------------------------------------------------------
\subsection{JupyterHub}
%------------------------------------------------------------------------------

CSCS provides a JupyterHub web portal for users to access Alps through Jupyter notebooks, which requires the Jupyter software stack installed on the cluster where the notebook will run.
The approach taken by CSCS is to create a simple \lst{jupyter} uenv that provides only jupyter, mounted at \lst{/user-tools}.

Two packages are installed, \lst{python} and \lst{py-pip}.
Then a ``post-install'' script, an optional script that can be used to modify the uenv right before it is compressed into a squashfs image, is used to install jupyter and other dependencies:

\lstinputlisting[language=bash]{code/jupyter-post-install.sh}

This uenv is automatically mounted by the JuputerHub service, alongside a uenv selected by the user, which provides the scientific software required.

%------------------------------------------------------------------------------
\subsection{Weather Service Production}
%------------------------------------------------------------------------------

CSCS hosts the operational cluster of The Swiss Weather Service (MeteoSwiss) on Alps -- a system with GPU nodes (4 $\times$ A100 GPU per node) for the weather forecast, and CPU only nodes (2 AMD Xeon CPU per node) for the other tasks in the operational weather forecast pipeline.

MCH require two distinct "programming environments":
\begin{enumerate}
    \item \emph{prgenv-nvidia} for building the atmospheric model ICON:
    \begin{itemize}
        \item the NVIDIA compiler toolchain, for compiling Fortran+OpenACCw
        \item libraries (HDF5, netcdf, etc).
        \item cray-mpich, cuda, etc.
    \end{itemize}
    \item \emph{prgenv-gcc} for building everything else:
    \begin{itemize}
        \item the gcc compiler toolchain, for compiling C, C++ and Fortran
        \item libraries (HDF5, netcdf, etc).
        \item cray-mpich
        \item python, R and ruby.
    \end{itemize}
\end{enumerate}
A single uenv provides both programming environments, that are configured using modules, views or Spack.

These images are also mounted permanantly, and versioned, e.g the latest version on the system is \lst{/mch-environment/v8}, where it is mounted for testing, while the current production runs are configured to use \lst{/mch-environment/v7}.
