In this section, we present a comprehensive analysis of CSCS's software deployment methodology, following the complete lifecycle from initial software compilation through to end-user interaction.
We examine three key stages: the building of software components with their specific dependencies and optimizations, the deployment mechanisms across diverse computational environments, and the user interface design that facilitates efficient access to these resources.
%This structured approach allows us to thoroughly address the complexities inherent in managing scientific software on large-scale HPC systems.

%------------------------------------------------------------------------------
\subsection{Building Software: Stackinator}
%------------------------------------------------------------------------------

Libraries that implement inter-node communication, for example MPI and NCCL, need to be optimized for the Slingshot 11 network in HPE Cray-EX systems.
The method used by \stackinator to install \craympich using Spack~\cite{gamblin:sc15} outside the CPE was demonstrated was shown in~\cite{uenv2023} 

Also of interest are the \cufftmp and \cusolvermp libraries from NVIDIA, that implement optimized distributed FFT and linear algebra solvers, with \nvshmem for distributed communication.
GROMACS and VASP are examples of widely-used scientific applications that can optionally be configured to use these libararies for the best performance.
However both \cufftmp and \cusolvermp are distributed by NVIDIA as pre-built binaries, intended for use on Infiniband networks.

This paper will document the additional steps required to build \nvshmem with libfabric support, and repackage and install \cufftmp and \cusolvermp to use this version of \nvshmem.
GROMACS built with \cufftmp will be be used to demonstrate and validate the performance of this approach.

%..............................................................................
\subsubsection{OpenMPI}
%..............................................................................

The main motivation behind providing OpenMPI is to have an alternative to \craympich, for when \craympich has application-specific performance problems or bugs, or users want to install a pre-compiled application that was linked against \openmpi.

In the paper we will explain the recipes and workflow for building different versions of the open source distributions of libfabric and the CXI driver from source, and integrating them in a uenv.
In the paper we will describe how we configure \openmpi with libfabric, and show performance results in applications up to thousands of Grace-Hopper GPUs.

%..............................................................................
\subsubsection{NVIDIA libraries}
%..............................................................................

NVIDIA provide a suite of mathematical and communication libraries for distributed computation.
Application developers have started integrating these libraries, where they are required for the best performance.

In particular, the \cufftmp and \cusolvermp libraries implement optimized distributed FFT and linear algebra solvers.
Examples of important applications that require these libraries for optimal performance are \cufftmp in GROMACS and \cusolvermp in VASP.
These two libraries use \nvshmem for distributed communication.
\nvshmem creates a global address space for data that spans the memory of multiple GPUs (both inter-node and intra-node) athat can be accessed via GPU-initiated operations.

Both \cufftmp and \cusolvermp are distrbuted as pre-compiled binary distributions.
\cufftmp in particular is challenging, because it is only distributed inside the NVIDIA HPC SDK, where it is linked against communication libraries that target UCX, not libfabric required for Slingshot 11.

The paper will describe in detail how CSCS provides these packages:
\begin{itemize}
    \item create a custom spack package to the latest version \nvshmem with support for libfabric/Slingshot 11.
    \item extract \cufftmp from the NVIDIA HPC SDK.
    \item write Spack packages that install \cufftmp and \cusolvermp and patch them to use \nvshmem built for libfabric.
    \item customise the Spack packages for GROMACS.
\end{itemize}

%------------------------------------------------------------------------------
\subsection{Deployment: CI/CD}
%------------------------------------------------------------------------------

Alps has five node node types described in \tbl{tab:alps-nodes}, on top of which CSCS deploys vClusters\footnote{versatile software defined cluster}~\cite{vClusters2023}, where each vCluster is a separate SLURM vCluster that is customised for specific platforms.

Each vCluster is configured for different target communities, use cases or customers.
As such, the version of SLURM, mounted file systems and installed software can vary between vClusters.

\begin{itemize}
    \item name: the name of the uenv
    \item version: the version of the uenv
    \item tag: used to indicate multiple releases of the same uenv
    \item uarch: the node type for which the image is built
    \item system: the vCluster for which the image is built
\end{itemize}

For example: \lst{prgenv-gnu/24.7:v1\%gh200@daint}

Is the gnu programming environment, version 24.7, tag v1, built for the Grace-Hopper nodes on Daint.

\begin{table*}[h!]
\centering
\begin{tabular}{llrrrr}
\toprule
uarch   & type         & blades & nodes & CPU sockets & GPU devices \\
\midrule
gh200   & NVIDIA GH200 & 1,344   & 2,688  & 10,752      & 10,752      \\
zen2    & AMD Rome     & 256     & 1,024  & 2,048       & --          \\
a100    & NVIDIA A100  & 72      & 144    & 144         & 576         \\
mi300   & AMD MI300A   & 64      & 128    & 512         & 512         \\
mi200   & AMD MI250x   & 12      & 24     & 24          & 96          \\
\midrule
\multicolumn{2}{c}{\textsc{Total}}      & 1,748   & 3,880  & 13,480  & 11,936 \\
\bottomrule
\end{tabular}
\caption{Alps node types and their specifications.}
\label{tab:alps-nodes}
\end{table*}

%------------------------------------------------------------------------------
\section{User Experience: CLI and SLURM}
%------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Providing CPE}
\label{sec:cpe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Though CPE is not used as the foundation of software environments, the CPE itself is an environment that we want to provide to users of Alps.

HPE has been making good progress improving the modularity of the CPE and providing more flexible methods of installing the CPE.

This section will describe how CSCS takes advantage of these efforts to provide ``CPE in a container", that delivers a familiar CPE user experience, while addressing the key challenges:

\begin{itemize}
\item CPE containers can be built, tested and deployed by the support team without root access;
\item New releases, custom configurations and patched releases of CPE can be deployed without any changes to the underlying system or interruption to service;
\item Using a container engine with full integration with SLURM simplifies the user experience
\end{itemize}

With the integrated container engine on Alps, starting a job with a container mounted is straightforward.
For example, to start an interactive shell on a compute node with a specific release of CPE loaded:
\begin{lstlisting}
srun --environment=cpe/24.11 --pty bash
\end{lstlisting}
