Installing and updating CPE requires root permission to rebuilding node image and reboot nodes with the new image, and updating the installation of CPE can break user-installed software built on top of the CPE.
This is in opposition to the objective of giving support staff the ability to deploy software stacks without root or interruption to the system.

CSCS continues to provide CPE via containers, built without root permission using Podman, deployed without modifying the cluster, and without removing previously deployed CPE containers.
This approach is possible due to two developments:
\begin{itemize}
       \item Alps uses forks of the \href{https://github.com/NVIDIA/enroot}{enroot} container runtime and \href{https://github.com/NVIDIA/pyxis}{Pyxis} Slurm plugin to provide an integrated container solution.
       \item HPE have improved the modularity of CPE packageing, and started releasing them through a new \href{https://cpe.ext.hpe.com/docs/latest/install/installation-guidance-container.html}{RPM repository} that can be used directly in Dockerfiles.
\end{itemize}

%The benefits of deploying CPE in a container include:
%\begin{itemize}
%\item CPE containers can be built, tested and deployed by the support team without root access;
%\item New releases, custom configurations and patched releases of CPE can be deployed without any changes to the underlying system or interruption to service;
%\end{itemize}

With the SLURM integration from Pyxis, starting a job with the CPE is simple, for example starting an interactive shell on a compute node with a specific release of CPE:
\begin{lstlisting}
srun --environment=cpe-24.7 --pty bash
\end{lstlisting}
The container environment is transparent to users, because the Slurm plugin mounts the scratch filesystem so that software built on scratch will be persist between invocations.

The paper will describe the workflow for building containers, and a link to a GitHub repository with DockerFiles use at CSCS, along with ReFrame tests for their deployment.
