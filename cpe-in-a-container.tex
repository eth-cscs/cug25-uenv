CSCS is phasing out using CPE as an underlying layer on top of which we build software for users, and we now recommend that users use either uenv or containers for building and running applications and workflows, for the reasons described in \sect{sec:cpe}.

CPE is still provided on our vClusters as containers.
At CSCS we have been testing monolithic containers that contain the full CPE, provided by HPE, since 2021.
These containers had the downside of being very large, and were pre-built which minimised the opportunities customization for CSCS' specific needs.

More recently, in 2024, HPE have started to release RPMs for CPE components in a repository that HPE customers can access, and providing recipes for building bespoke CPE containers\footnote{\href{https://cpe.ext.hpe.com/docs/latest/install/installation-guidance-container.html}{\lst{cpe.ext.hpe.com}}}.
In this section we will provide detailed instructions on how CSCS builds CPE containers, and links to repositories with the Containerfiles, followed by a discussion about how the containers are deployed and used on Alps.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Creating CPE containers}
\label{sec:cpe-container-create}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Getting access to the RPM repository provided by HPE one needs an HPE Passport account, and generate a token\footnote{\href{https://cpe.ext.hpe.com/docs/latest/install/token-authed-repo.html}{\lst{cpe.ext.hpe.com/docs/latest/install/token-authed-repo.html}}}.
\todo{full link as footnote? fake linke like e.g. cpe.ext.hpe.com/token-authed-repo? No link at all?}

HPE provides CPE packages for both, Redhat Enterprise Linux and SUSE Linux Enterprise, where newer versions of CPE are provided for both architectures, x86\_64 and aarch64.
Once we have an account with HPE, generated an access token and settled on the operating system, we can set up the package manager to pull packages from HPE's package repository, by pointing it to the URL \lstinline{https://update1.linux.hpe.com/repo/cpe/<CPE-VER>/base/<OS>/<OS-VER>/<ARCH>}, where

\begin{itemize}
    \item \emph{\textless CPE-VER\textgreater} is the CPE version, e.g. 24.7, or 25.3
    \item \emph{\textless OS\textgreater} is the operating system, i.e. \lst{rhel} or \lst{sle}
    \item \emph{\textless OS-VER\textgreater} is the operating systems's version, e.g. \lst{15.5}
    \item \emph{\textless ARCH\textgreater} \lst{x86_64} or \lst{aarch64}, optionally needed (CPE-24.7 requires the flag, CPE-25.3 has an architecture agnostic repository setup)
\end{itemize}

When building CPE in a container with GPU support, it is necessary to install also NVIDIA CUDA, which is provided as an RPM repository by NVIDIA.
A \lst{Dockerfile} to build CPE in a container can be found at \lstinline{https://github.com/eth-cscs/cpe-containers}. The Dockerfile is parametrized to allow installing different versions of CPE,
Every CPE release requires slightly varying packages to be installed, mostly the versions need to be adapted.
We are building CPE container images for the GNU and Cray programming environment.
Each programming environment will have its own container image.
This allows to load default modules, when the container is instantiated, such that the programming environment is directly available without explicit \lst{module load PrgEnv-gnu}.
The default list of modules loaded at startup is also parametrized in the Dockerfile, and the default is to load the most basic ones, which allows to compile applications for the target architecture. This is the module list for \lst{cpe-gnu} on the Grace Hopper nodes
\begin{itemize}
    \item craype-arm-grace
    \item craype-network-ofi
    \item craype
    \item xpmem
    \item PrgEnv-gnu
    \item cray-mpich
    \item cuda
    \item craype-accell-nvidia90
\end{itemize}
Another advantage of building one container image per programming environment is the reduced image size of the image.
Admittedly the sum of \lst{cpe-gnu} and \lst{cpe-cray} container images is larger than building both in the same container, because some packages, especially the cuda-toolkit, is installed in each container image.
However a smaller size per programming environment will have long term advantages, where user's typically are only interested in one environment, and will build their container image on top of the programming environment of their interest.


\todo{AF: I think I addressed all points, so please remove if you think so too}
\todo{summarize the process for creating CPE containers}
\begin{itemize}
    \item how to get access to the RPM repository and pull from it
    \item how we set up the dockerfile/containerfile
    \item how we configured which modules to load automatically to set up the "environment"
    \item how we chose to split into separate GNU and Cray images instead of a "one size fits all image"
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deploying CPE containers}
\label{sec:cpe-container-deploy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the repository \lstinline{https://github.com/eth-cscs/cpe-containers} we have a CI/CD pipeline to build, test and deploy the container images, for the different CPE versions and programming environments.
The pipeline is building a fresh container image, when opening a PR, which is also tested on the target system.
On a merge to the \lst{main} branch the container image is also deployed to the CSCS container registry.
Once the image is deployed to the registry, it can be pulled on the target system, and made available for users.


\todo{Move cpe-containers repo}
Should we move the \lstinline{https://github.com/finkandreas/cpe-containers/} repo to the \lstinline{eth-cscs} organisation?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{using CPE containers}
\label{sec:cpe-container-use}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo{describe how the containers are used.}

\begin{itemize}
       \item Alps uses forks of the \href{https://github.com/NVIDIA/enroot}{enroot} container runtime and \href{https://github.com/NVIDIA/pyxis}{Pyxis} Slurm plugin to provide an integrated container solution.
       \item HPE have improved the modularity of CPE packageing, and started releasing them through a new \href{https://cpe.ext.hpe.com/docs/latest/install/installation-guidance-container.html}{RPM repository} that can be used directly in Dockerfiles.
\end{itemize}
With the SLURM integration from Pyxis, starting a job with the CPE is simple, for example starting an interactive shell on a compute node with a specific release of CPE:
\begin{lstlisting}
srun --environment=cpe-24.7 --pty bash
\end{lstlisting}
The container environment is transparent to users, because the Slurm plugin mounts the scratch filesystem so that software built on scratch will persist between invocations.
The instantiation of the container is very much equivalent of a \lstinline{module load PrgEnv-gnu}. Additional modules like \lst{hdf5}, if needed, can be loaded by the user.

