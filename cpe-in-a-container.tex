CSCS is phasing out using CPE as an underlying layer on top of which we build software for users, and we now recommend that users use either uenv or containers for building and running applications and workflows, for the reasons described in \sect{sec:cpe}.

CPE is still provided on our vClusters as containers.
At CSCS we have been testing monolithic containers that contain the full CPE, provided by HPE, since 2021.
These containers had the downside of being very large, and were pre-built which minimised the opportunities customization for CSCS' specific needs.

More recently, in 2024, HPE have started to release RPMs for CPE components in a repository that HPE customers can access, and providing recipes for building bespoke CPE containers\footnote{\href{https://cpe.ext.hpe.com/docs/latest/install/installation-guidance-container.html}{\lst{cpe.ext.hpe.com}}}.
In this section we will provide detailed instructions on how CSCS builds CPE containers, and links to repositories with the Containerfiles, followed by a discussion about how the containers are deployed and used on Alps.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Creating CPE containers}
\label{sec:cpe-container-create}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo{summarize the process for creating CPE containers}
\begin{itemize}
    \item how to get access to the RPM repository and pull from it
    \item how we set up the dockerfile/containerfile
    \item how we configured which modules to load automatically to set up the "environment"
    \item how we chose to split into separate GNU and Cray images instead of a "one size fits all image"
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deploying CPE containers}
\label{sec:cpe-container-deploy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Describe the CI/CD pipeline.

Should we move the \lstinline{https://github.com/finkandreas/cpe-containers/} repo to the \lstinline{eth-cscs} organisation?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{using CPE containers}
\label{sec:cpe-container-use}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo{describe how the containers are used.}

\begin{itemize}
       \item Alps uses forks of the \href{https://github.com/NVIDIA/enroot}{enroot} container runtime and \href{https://github.com/NVIDIA/pyxis}{Pyxis} Slurm plugin to provide an integrated container solution.
       \item HPE have improved the modularity of CPE packageing, and started releasing them through a new \href{https://cpe.ext.hpe.com/docs/latest/install/installation-guidance-container.html}{RPM repository} that can be used directly in Dockerfiles.
\end{itemize}
With the SLURM integration from Pyxis, starting a job with the CPE is simple, for example starting an interactive shell on a compute node with a specific release of CPE:
\begin{lstlisting}
srun --environment=cpe-24.7 --pty bash
\end{lstlisting}
The container environment is transparent to users, because the Slurm plugin mounts the scratch filesystem so that software built on scratch will be persist between invocations.

