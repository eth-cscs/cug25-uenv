The  \stackinator tool, that is used to generate squashfs images from a YAML-based recipe, was documented in detail at CUG 2023~\cite{uenv2023}.
The implementation details of \stackinator were covered in that paper, and have not changed materially.
The method used by \stackinator to install \craympich using Spack~\cite{gamblin:sc15} outside the CPE was demonstrated was shown in~\cite{uenv2023}
This approach is used by \stackinator, however it can also be used directly with Spack -- CSCS staff use it to install the most recent versions of \craympich on LUMI.

\todo{add some more color about stackinator: changes since 2023; per-stack spack versioning; site repo; how all this minimises the Spack upgrade pain.}
\subsubsection{Customizing networking}
\label{sec:networking}
Although support for \craympich is mature and well tested, it relies on an external package installed via RPM,
the recent release of source codes for \slingshot libraries (\lst{libcxi}, \lst{cxi-drivers}, \lst{cassini-headers}) and their associated libfabric \lst{cxi} provider,
gives us the incentive to provide more flexibility in networking layer customization, including:
\begin{itemize}
    \item linking \craympich to non default (compiled from source) versions of \slingshot dependencies
    \item supporting non-vendor provided MPI implementations such as OpenMPI
    \item allowing custom versions/feature branches and options for both top level (MPI) and dependent libraries (\lst{libfabric}, \lst{cxi}, etc)
\end{itemize}
To achieve this, we have extended the \lst{mpi} section of the \stackinator YAML recipe so that users can override the defaults setup by cluster managers. The following extract (fig \ref{lst:openmpi-config}) shows how one can redefine the networking stack using the familiar spack syntax (note that yaml tags shown may change prior to next release). When combined with \stackinator's existing \lst{package} customizations (where \lst{package_attributes} such as \lst{git} repository/location may be set), the new options give total control over the network configuration on a per cluster basis.
\begin{figure}[htp!]
    \lstinputlisting[language=yaml]{code/openmpi.yaml}
    \caption{Specifying a custom developer version of OpenMPI}
    \label{lst:openmpi-config}
\end{figure}
A user may leave the default \craympich version, but override the \lst{depends} section to change drivers, or override the \lst{mpi} spec entirely as well as providing custom versions of any or all of the dependencies

Currently the \slingshot sources must be tested carefully for compatibility since official release versions are not regularly made,
git SHA refs are used to identify versions used in \stackinator defaults. As the recipes become more widely used and tested, concrete versions will be chosen for stable uenv deployment.

\todo{NVIDIA libraries}

This section will focus on Spack packing for other key libraries that implement inter-node communication -- for example MPI, NCCL and \nvshmem -- need to be optimized for the Slingshot 11 network in HPE Cray-EX systems.

We will focus on three main sets of libraries. The first is building OpenMPI with libfabric support, in order to provide an alternative to \craympich when there are bugs or performance regressions for specific applications, and to support software that is distributed as binaries linked against OpenMPI. The second is how to build the open source libfabric and CXI driver software for Slingshot 11. Finally, we will show how we adapt \cufftmp and \cusolvermp, which are distributed as pre-build binaries for infiniband, to use \nvshmem with libfabric support, which we developed in collaboration with NVIDIA.

Note that while these methods are integrated into Stackinator, they can be used directly by Spack. All Spack packages, scripts and guides will be made available on GitHub for readers to use.
