CSCS is phasing out using CPE as an underlying layer on top of which we build software for users, and we now recommend that users use either uenv or containers for building and running applications and workflows, for the reasons described in \sect{sec:cpe}.
However, we want to continue providing CPE in a way that meets the objectives in \sect{sec:objectives}.

Containers can meet the objectives, because container images can be built and modified by staff and deployment pipelines without root permission; can be pulled and run without modifying the underlying cluster; and users are not forced to upgrade when a new version is released.

At CSCS we have been testing monolithic containers that contain the full CPE, provided by HPE, since 2021.
These containers had the downside of being very large, and were pre-built which minimised the opportunities customization for CSCS' specific needs.

More recently, in 2024, HPE have started to release RPMs for CPE components in a repository that HPE customers can access, and providing recipes for building bespoke CPE containers\footnote{\href{https://cpe.ext.hpe.com/docs/latest/install/installation-guidance-container.html}{\lst{cpe.ext.hpe.com/docs/latest/install/installation-guidance-container.html}}}.
In this section we will provide detailed instructions on how CSCS builds CPE containers, and links to repositories with the Containerfiles, followed by a discussion about how the containers are deployed and used on Alps.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Creating CPE containers}
\label{sec:cpe-container-create}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To access the RPM repository one needs an HPE Passport account, and generate a token\footnote{\href{https://cpe.ext.hpe.com/docs/latest/install/token-authed-repo.html}{\lst{cpe.ext.hpe.com/docs/latest/install/token-authed-repo.html}}}.
Once we have an account with HPE, generated an access token and settled on the operating system, we can set up the package manager to pull packages from HPE's package repository, by pointing it to the URL \lstinline{https://update1.linux.hpe.com/repo/cpe/<CPE-VER>/base/<OS>/<OS-VER>/<ARCH>}, where

\begin{itemize}
    \item \emph{\textless CPE-VER\textgreater} is the CPE version, e.g. 24.7, or 25.3
    \item \emph{\textless OS\textgreater} is the operating system, i.e. \lst{rhel} or \lst{sle}
    \item \emph{\textless OS-VER\textgreater} is the operating systems's version, e.g. \lst{15.5}
    \item \emph{\textless ARCH\textgreater} \lst{x86_64} or \lst{aarch64}, optionally needed (CPE-24.7 requires the flag, CPE-25.3 has an architecture agnostic repository setup)
\end{itemize}

When building CPE in a container with GPU support, it is necessary to install also NVIDIA CUDA, which is provided as an RPM repository by NVIDIA.
A \lst{Dockerfile} to build CPE in a container can be found on CSCS' GitHub\footnote{\href{https://github.com/eth-cscs/cpe-containers}{\lstinline{github.com/eth-cscs/cpe-containers}}}. The Dockerfile is parametrized to allow installing different versions of CPE,
Every CPE release requires slightly varying packages to be installed, mostly the versions need to be adapted.
We are building CPE container images for the GNU and Cray programming environment, with one container image for each.
This allows us to load default modules when the container is instantiated, such that the programming environment is directly available without, for example, typing \lst{module load PrgEnv-gnu}.

The default list of modules loaded at startup is also parametrized in the Dockerfile, with the defaults chosen to initialise an environment that is ready to compile applications for the target architecture.
For example, this is the module list for \lst{cpe-gnu} on the Grace Hopper nodes is:
\begin{itemize}
    \item craype-arm-grace
    \item craype-network-ofi
    \item craype
    \item xpmem
    \item PrgEnv-gnu
    \item cray-mpich
    \item cuda
    \item craype-accell-nvidia90
\end{itemize}
Note that the craype-accell-nvidia90 module is loaded, unlike the CPE where users have to explicitly load it, which reduced the number of user tickets complaining about the ``GPU\_SUPPORT\_ENABLED is requested, but GTL library is not linked'' errors.

Another advantage of building one container image per programming environment is the reduced image size of the image.
Admittedly the sum of \lst{cpe-gnu} and \lst{cpe-cray} container images is larger than building both in the same container, because some packages like cuda-toolkit, are installed in each container image.
However a smaller size per programming environment will have long term advantages, where users typically are only interested in one environment, and will build their container image on top of the programming environment of their interest.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deploying CPE containers}
\label{sec:cpe-container-deploy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the repository \href{https://github.com/eth-cscs/cpe-containers}{\lstinline{eth-cscs/cpe-containers}} GitHub repository we have a CI/CD pipeline to build, test and deploy the container images for the different CPE versions and programming environments.
The pipeline builds a new container image when a PR is opened, which is also tested on the target system.
On a merge to the \lst{main} branch the container image is then deployed to the CSCS container registry.
Once the image is deployed to the registry, it can be pulled on the target system, and made available for users.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{using CPE containers}
\label{sec:cpe-container-use}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Alps uses forks of the \href{https://github.com/NVIDIA/enroot}{enroot} container runtime and \href{https://github.com/NVIDIA/pyxis}{Pyxis} \slurm plugin to provide an integrated container solution.
With the \slurm integration from Pyxis, starting a job with the CPE is simple, for example starting an interactive shell on a compute node with a specific release of CPE:
\begin{lstlisting}
srun --environment=cpe-gnu-24.7 --pty bash
\end{lstlisting}
The container environment is transparent to users, because the \slurm plugin mounts the scratch filesystem so that software built on scratch will persist between invocations.
The instantiation of the container is equivalent to \lstinline{module load PrgEnv-gnu}.
